{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4984f2c8",
   "metadata": {},
   "source": [
    "# some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd681151",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatplotlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2285\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2283\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2285\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/magics/pylab.py:99\u001b[0m, in \u001b[0;36mPylabMagics.matplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable matplotlib backends: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m backends_list)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     gui, backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_matplotlib_backend(args\u001b[38;5;241m.\u001b[39mgui, backend)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3436\u001b[0m, in \u001b[0;36mInteractiveShell.enable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable_matplotlib\u001b[39m(\u001b[38;5;28mself\u001b[39m, gui\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3416\u001b[0m     \u001b[38;5;124;03m\"\"\"Enable interactive matplotlib and inline figure support.\u001b[39;00m\n\u001b[1;32m   3417\u001b[0m \n\u001b[1;32m   3418\u001b[0m \u001b[38;5;124;03m    This takes the following steps:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3434\u001b[0m \u001b[38;5;124;03m        display figures inline.\u001b[39;00m\n\u001b[1;32m   3435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3436\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib_inline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_inline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_inline_support\n\u001b[1;32m   3438\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pylabtools \u001b[38;5;28;01mas\u001b[39;00m pt\n\u001b[1;32m   3439\u001b[0m     gui, backend \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mfind_gui_and_backend(gui, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpylab_gui_select)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"A matplotlib backend for publishing figures via display_data\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (c) IPython Development Team.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Distributed under the terms of the BSD 3-Clause License.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_agg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     new_figure_manager,\n\u001b[1;32m      9\u001b[0m     FigureCanvasAgg,\n\u001b[1;32m     10\u001b[0m     new_figure_manager_given_figure,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.generate dataset\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "# plot dataset\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y) \n",
    "plt.legend([\"Class 0\", \"Class 1\"], loc=4) \n",
    "plt.xlabel(\"First feature\") \n",
    "plt.ylabel(\"Second feature\")\n",
    "print(\"X.shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.e.g. Plot of the wave dataset\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3280d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.e.g.cancer\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "cancer = load_breast_cancer() \n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    "\n",
    "print(\"the shape of cancer data: {}\".format(cancer.data.shape))\n",
    "print(\"sample counts per class: \\n{}\".format({\n",
    "    n:v for n,v in zip(cancer.target_names, np.bincount(cancer.target))\n",
    "}))\n",
    "print(\"Feature names: \\n{}\".format(cancer.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.e.g.house price\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(\"Data shape: {}\".format(boston.data.shape))\n",
    "# load the derived data (feature engineering)\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "print(\"X.shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12240d77",
   "metadata": {},
   "source": [
    "# 1 k-Nearest Neighbors\n",
    "## 1.1 plot to show the case of classification on the forge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a759542",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_classification(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_classification(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6c76d",
   "metadata": {},
   "source": [
    "## 1.2 splite the data to train the model and evaluate the generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y = mglearn.datasets.make_forge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# instantiate the class, set the parameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fit the classifier using the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make prediction\n",
    "print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n",
    "\n",
    "# evaluation\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fddace",
   "metadata": {},
   "source": [
    "## 1.3 Analyzing KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d98aa",
   "metadata": {},
   "source": [
    "### 1.3.1 visualizations of the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 1 neighbors\n",
    "fig, axes = plt.subplots(1,3,figsize=(10,3))\n",
    "\n",
    "for n_neighbors, ax in zip([1,3,9], axes):\n",
    "     # the fit method returns the object self, so we can instantiate and fit in one line\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X,y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    "    ax.set_xlabel(\"feature 0\")\n",
    "    ax.set_ylabel(\"feature 1\")\n",
    "    axes[0].legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3136a",
   "metadata": {},
   "source": [
    "## 1.4 investigate the connection between model complexity and generalization: Breast Cancer datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "training_accuracy = [] \n",
    "test_accuracy = []\n",
    "\n",
    "# try n_neighbors from 1 to 10 \n",
    "neighbors_settings = range(1, 11)\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors) \n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy \n",
    "    training_accuracy.append(clf.score(X_train, y_train)) \n",
    "    # record generalization accuracy \n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55d158",
   "metadata": {},
   "source": [
    "## 1.5 k-neighbors regression (a regression variant of the k-nearest neighbors algorithm): wave data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3225ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X,y = mglearn.datasets.make_wave(n_samples=40)\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# instantiate the model and set the number of neighbors to consider to 3\n",
    "reg = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# fit the model using the training data and training targets \n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# make prediction on the data set\n",
    "print(\"test set predictions:\\n{}\".format(reg.predict(X_test)))\n",
    "\n",
    "# evaluation\n",
    "print(\"test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d78568",
   "metadata": {},
   "source": [
    "## 1.6 Analyzing KNeighborsRegressor\n",
    "to visualize the prediction of all posible feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(15,4))\n",
    "\n",
    "# create 1,000 data points, evenly spaced between -3 and 3\n",
    "line = np.linspace(-3,3,1000).reshape(-1,1)\n",
    "for n_neighbors, ax in zip([1,3,9], axes):\n",
    "    # make predictions using 1,3 or 9 neighbors\n",
    "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    reg.fit(X_train, y_train)\n",
    "    ax.plot(line, reg.predict(line))\n",
    "    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n",
    "    ax.plot(X_test, y_test, '^', c=mglearn.cm2(1), markersize=8)\n",
    "    \n",
    "    ax.set_title(\n",
    "    \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n",
    "    n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)\n",
    "    )\n",
    "    )\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Target\")\n",
    "    \n",
    "axes[0].legend([\"Model predictions\", \"Training data/target\", \"Test data/target\"], loc = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59316b",
   "metadata": {},
   "source": [
    "## 1.7 Strengths, weaknesses, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05700cf",
   "metadata": {},
   "source": [
    "# 2 Linear Models\n",
    "## 2.1 Linear models for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353b28d",
   "metadata": {},
   "source": [
    "### 2.1.1 Linear regression (aka ordinary least squares (OLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144211c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"The coefficient, lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"The intercept, lr.intercept_: {}\".format(lr.intercept_))\n",
    "\n",
    "# performance:\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5a6f8",
   "metadata": {},
   "source": [
    "How the OLS perform on the more complex dataset: Boston Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr =LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "# it shows the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024017a",
   "metadata": {},
   "source": [
    "### 2.1.2 Ridge regression\n",
    "it allows us to control complexity of the model to **avoid overfitting** when using the **standar linear regression** by **L2 regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb36e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3121d39",
   "metadata": {},
   "source": [
    "**alpha parameter** \\\n",
    "alpha &darr; &rarr; coefficients to be less ristricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b737d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fb65d",
   "metadata": {},
   "source": [
    "**when the alpha is little enough, it ends up with a model that resembles *LinearRegression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46369cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha = 0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2313a4",
   "metadata": {},
   "source": [
    "**The relationship between the higher alpha leads to the coeficient with smaller magnitude of value**: `coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridge.coef_, 's', label = \"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Codfficient magnitude\")\n",
    "plt.hlines(0,0,len(lr.coef_))\n",
    "plt.ylim(-25,25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab97b57",
   "metadata": {},
   "source": [
    "change the amout of data rather than the alpha to see **the influence of regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_ridge_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7bddf7",
   "metadata": {},
   "source": [
    "### 2.1.3 Lasso\n",
    "an alternative to **Ridge** for regularizing linear regression: **L1 regularization** \\\n",
    "the **regularization parameter: alpha** of this model means: how strongly coefficients are pushed toward 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53bac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test))) \n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa650ce",
   "metadata": {},
   "source": [
    "alpha &darr; & `max_iter` &uarr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we increase the default setting of \"max_iter\",\n",
    "# otherwise the model would warn us that we should increase max_iter. \n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) \n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test))) \n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00e042",
   "metadata": {},
   "source": [
    "alpha &darr; &rarr; regularization&darr; &rarr; LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2482214b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lasso' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_d/ygp6b6291jq_99s9fqh98qv40000gn/T/ipykernel_6975/1062120360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlasso00001\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training set score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso00001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso00001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of features used: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso00001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lasso' is not defined"
     ]
    }
   ],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train) \n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test))) \n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6061af",
   "metadata": {},
   "source": [
    "plot the coefficient of different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596d773",
   "metadata": {},
   "source": [
    "## 2.2 Linear model for classification\n",
    "### 2.2.1 Binary Classification: *LogisticRegression & linear support vector machines* (linear SVMs) (both L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X,y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize = (10,3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill = False, eps = 0.5, ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:,0], X[:,1], y, ax=ax)\n",
    "    ax.set_title(\"{}\".format(clf.__class__.__name__)) \n",
    "    ax.set_xlabel(\"Feature 0\") \n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "    axes[0].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fab3fb",
   "metadata": {},
   "source": [
    "**the parameter C:** &uarr; C &rarr; less regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47529719",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42\n",
    ")\n",
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042bd83",
   "metadata": {},
   "source": [
    "increase C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59368b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100, max_iter = 10000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01, max_iter=1000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1080a0a",
   "metadata": {},
   "source": [
    "the coefficient of three models with different parameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f0be9",
   "metadata": {},
   "source": [
    "coefficient plot and classification accuracies for L1 regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is some problem about the penalty in lr_l1, in the textbook, its value is l1\n",
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l2\", max_iter = 10000).fit(X_train, y_train) \n",
    "    \n",
    "    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "        C, lr_l1.score(X_train, y_train)))\n",
    "    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "        C, lr_l1.score(X_test, y_test)))\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "    \n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2258d5",
   "metadata": {},
   "source": [
    "### 2.2.2 multiclass classification: one-vs.-rest & multiclass logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d884d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape) \n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee568a7",
   "metadata": {},
   "source": [
    "### Visualize the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                      ['b', 'r', 'g']):\n",
    "        plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1b995",
   "metadata": {},
   "source": [
    "### prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d230604",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7) \n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                      ['b', 'r', 'g']):\n",
    "        plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34897d",
   "metadata": {},
   "source": [
    "## 2.3 Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494332",
   "metadata": {},
   "source": [
    "GaussianNB &rarr; continuous data \\\n",
    "BernoulliNB &rarr; binary data \\\n",
    "MultinomialNB &rarr; count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ae3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # iterate over each class\n",
    "    # count (sum) entries of 1 per feature \n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"Feature counts:\\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632662e",
   "metadata": {},
   "source": [
    "## 2.4 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fe16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_animal_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a975e",
   "metadata": {},
   "source": [
    "### 2.4.1 Controlling complexity of decision trees: prune the tree (limit the depth):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42\n",
    ")\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e77f2",
   "metadata": {},
   "source": [
    "### 2.4.2 Analysing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "               feature_names=cancer.feature_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "with open(\"tree.dot\") as f: \n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f713c1",
   "metadata": {},
   "source": [
    "### 2.4.3 Feature importance in trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ea63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = mglearn.plots.plot_tree_not_monotone()\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79834f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b047eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# use historical data to forecast prices after the year 2000 \n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "    \n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target \n",
    "y_train = np.log(data_train.price)\n",
    "    \n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7929c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded89b6d",
   "metadata": {},
   "source": [
    "## 2.5 Ensembles of Decidion Trees (avoid overfitting)\n",
    "used in place of a single decision tree\n",
    "### 2.5.1 Random forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52688e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "        ax.set_title(\"Tree {}\".format(i))\n",
    "        mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
    "                                alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddab278",
   "metadata": {},
   "source": [
    "##### e.g. Random forest fitting the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c43c0",
   "metadata": {},
   "source": [
    "### 2.5.2 gradient boosted decision trees (gradient boosting machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3745b12",
   "metadata": {},
   "source": [
    "#### to reduce the overfit -- strong prepruning: *limit the maximum depth* or *lower the learning rate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816f1ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_d/ygp6b6291jq_99s9fqh98qv40000gn/T/ipykernel_6975/783273823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgbrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgbrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on training set: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on test set: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0227c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b38710",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41d191",
   "metadata": {},
   "source": [
    "## 2.6 Kernelized Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c71df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=8) \n",
    "y=y%2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC \n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    " \n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdef6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d \n",
    "figure = plt.figure()\n",
    "\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "\n",
    "# plot first all the points with y == 0, then all with y == 1\n",
    "mask=y==0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',cmap=mglearn.cm2, s=60)\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature0 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af18c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ=YY**2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()]) \n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "                 cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y) \n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "# class labels of support vectors are given by the sign of the dual coefficients \n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3) \n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad726",
   "metadata": {},
   "source": [
    "### 2.4.2 Tuning SVM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a130fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "\n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"], ncol=4, loc=(.9, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d7274",
   "metadata": {},
   "source": [
    "#### Apply the RBF kernel SVM to the breast Cancer dataset. (C=1, gamma = 1/n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c460557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
    "plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61274a0",
   "metadata": {},
   "source": [
    "#### Preprocessing data for SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e140d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum value per feature on the training set\n",
    "min_on_training = X_train.min(axis=0)\n",
    "\n",
    "# compute the range of each feature (max - min) on the training set \n",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "\n",
    "# subtract the min, and divide by range\n",
    "# afterward, min=0 and max=1 for each feature\n",
    "X_train_scaled = (X_train - min_on_training) / range_on_training \n",
    "print(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0))) \n",
    "print(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use THE SAME transformation on the test set,\n",
    "# using min and range of the training set (see Chapter 3 for details) \n",
    "X_test_scaled = (X_test - min_on_training) / range_on_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format( svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6a39c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_d/ygp6b6291jq_99s9fqh98qv40000gn/T/ipykernel_6975/2857783024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on training set: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on test set: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format( svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56eb0a",
   "metadata": {},
   "source": [
    "## 2.7 Neural Networks (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6701d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mglearn.plot_logistic_regression_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156a077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f62d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
